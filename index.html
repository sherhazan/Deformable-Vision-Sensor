<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A robust learning-based 3D vision sensor designed for estimating the relative position between both rigid and deformable objects in real-time using Late Fusion CNN.">
  <meta name="keywords" content="Robotic Assembly, Peg-in-Hole, Deformable Objects, Computer Vision, Late Fusion, RGB-D">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Learning-Based 3D Vision Sensor for Robotic Peg-In-Hole Insertion of Deformable Objects</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg">  -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Learning-Based 3D Vision Sensor for Robotic Peg-In-Hole Insertion of Deformable Objects</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Sher Hazan,</span>
            <span class="author-block">Eylon Cohen,</span>
            <span class="author-block">Ronit Schneor,</span>
            <span class="author-block">Anath Fischer,</span>
            <span class="author-block">Miriam Zacksenhouse</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Technion - Israel Institute of Technology</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://sherhazan.github.io/Deformable-Vision-Sensor/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/YOUR_ARXIV_ID"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/sherhazan/LDO-robotic-insertion"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Real-time robotic control using our 3D Vision Sensor. The system robustly estimates the relative position of the deformable medical pipe to ensure successful insertion.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The integration of precise object positioning systems in robotic assembly tasks, particularly Peg-in-Hole (PIH) operations, is crucial for enhancing automation efficiency in industrial environments. The challenge intensifies when dealing with deformable objects. Advanced sensors and learning methods are recruited to face such challenges. This research presents a robust learning-based 3D vision sensor which is designed for estimating the relative position between both rigid and deformable objects in real-time. It is planned to be embedded into a robotic PIH insertion pipeline in the industrial environment. Our 3D vision sensor incorporates capturing the 3D scene, predicting a probability vector indicating the relative position between the peg and hole. It uses a late fusion convolutional neural network (CNN) architecture for classification, extracting the continuous relative position from the probabilities vector. The estimated relative position is then utilized to refine the peg’s position during the insertion process. The efficiency of the 3D vision sensor was evaluated across various PIH tasks involving both rigid and deformable objects. Moreover, it was embedded into an industrial robotic cell without additional training in order to validate its robustness and generalization capabilities. The approach demonstrated high success rate, highlighting its effectiveness and applicability in real-world industrial settings.
          </p>
        </div>
      </div>
    </div>
    </div>
</section>

<section class="section" style="padding-top: 0;">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method</h2>
        <h3 class="title is-4">PIH Insertion Pipeline</h3>
        <div class="content has-text-justified">
          <p>
            We propose a robust pipeline for robotic Peg-in-Hole (PIH) insertion that integrates our 3D vision sensor with force/torque feedback. The process is iterative and designed to handle both rigid and deformable objects in industrial environments. The pipeline consists of the following iterative stages:
          </p>
          <p>
            <strong>Pipeline Stages:</strong>
          </p>
          <ul>
            <li>
              <strong>1. 3D Vision Sensor:</strong> The system captures the current 3D scene and estimates the relative position between the peg and the hole in terms of relative radius ($\hat{R}$) and relative angle ($\hat{\theta}$).
            </li>
            <li>
              <strong>2. Virtual F/T Sensor:</strong> Raw force ($F_S$) and torque ($T_S$) measurements from the OnRobot sensor are transformed to a virtual point ($P_V$) located at the center of the peg's tip, with its Z-axis aligned to the insertion direction.
            </li>
            <li>
              <strong>3. Virtual Friction Force Sensor:</strong> The friction components of the force vector are replaced with a vision-based virtual friction force ($\hat{F}$). This force represents the direction of the estimated relative angle ($\hat{\theta}$), helping to guide the robot.
            </li>
            <li>
              <strong>4. Decision Making:</strong> The system checks the estimated relative radius ($\hat{R}$). If it exceeds a predefined threshold (determined by the object's rigidity), the system proceeds to location improvement. If the condition is met ($\hat{R} \le Threshold$), the insertion is initiated.
            </li>
            <li>
              <strong>5. Location Improvement:</strong> A position-based impedance controller adjusts the robot's location. It optimizes the trade-off between minimizing the position error (peg vs. hole) and minimizing the virtual forces and torques.
            </li>
            <li>
              <strong>6. Insertion:</strong> Once the alignment is confirmed by the decision-making module, the robotic arm performs the final insertion.
            </li>
          </ul>
        </div>
          <figure class="image is-centered" style="margin-bottom: 30px;">
            <img src="./static/images/pipeline_diagram.png" 
                  alt="3D Vision Sensor Block Diagram" 
                  style="width: 80%; display: block; margin-left: auto; margin-right: auto;"/>
            <figcaption class="has-text-centered is-size-7">Block diagram of the entire PIH insertion pipeline embedded with the 3D vision sensor.</figcaption>
          </figure>
      </div>
    </div>

      <div class="columns is-centered">
      <div class="column is-full-width">
        <h3 class="title is-4">3D Vision Sensor</h3>
        
        <div class="content has-text-justified">
          <p>
            The proposed 3D vision sensor utilizes RGB-D data, classification CNN, and linear interpolation to estimate the relative angle (\(\hat{\theta}\)) and radius (\(\hat{R}\)) between the peg and the hole in the insertion plane. The sensor captures the 3D scene and predicts a probability vector indicating the relative position, which is then extracted to refine the peg's position during the insertion process.
          </p>
          </div>

          <figure class="image is-centered" style="margin-bottom: 30px;">
            <img src="./static/images/vision_sensor_block_diagram.png" 
                 alt="3D Vision Sensor Block Diagram" 
                 style="width: 80%; display: block; margin-left: auto; margin-right: auto;"/>
            <figcaption class="has-text-centered is-size-7">Block diagram of the proposed 3D vision sensor for estimating relative positions in Peg-in-Hole tasks.</figcaption>
          </figure>

          <p>
            In order to apply classification methods to assembly tasks within a continuous domain, a discretization of the scene space is required. This is achieved by dividing the scene space into equal sub-areas as classes, where each class corresponds to a range of values in the domain. Following the discretization, a classification CNN is utilized to predict the probability vector of the peg being in each sub-area.
          </p>

          <figure class="image is-centered" style="margin-bottom: 30px;">
            <img src="./static/images/discretization.png" 
                 alt="Space Discretization" 
                 style="width: 80%; display: block; margin-left: auto; margin-right: auto;"/>
            <figcaption class="has-text-centered is-size-7">Discretization of the scene space into sub-areas: (a) according to angle, (b) according to radius.</figcaption>
          </figure>

          <p>
            For this task, a late RGB-D fusion architecture was proposed utilizing two separate ResNet-18 CNNs, one for RGB images and one for depth data. The features from the convolutional layers of both CNNs are concatenated before reaching the fully connected layer. This architecture allows the model to leverage both visual and geometric features effectively.
          </p>

          <figure class="image is-centered" style="margin-bottom: 30px;">
            <img src="./static/images/vision_sensor_architecture.png" 
                 alt="Late Fusion CNN Architecture" 
                 style="width: 80%; display: block; margin-left: auto; margin-right: auto;"/>
            <figcaption class="has-text-centered is-size-7">Architecture of the proposed RGB-D late fusion model using separate RGB and depth ResNet-18 CNNs, followed by feature concatenation and a fully connected layer.</figcaption>
          </figure>

          <p>
            The performance of the proposed model was compared to early and parallel fusion architectures, and it was found that the late fusion architecture yields the best performance. The predicted probability vector is employed to estimate the continuous relative angle and radius through linear interpolation. This process involves thresholding negligible values to suppress noise, normalizing the probability vector, and calculating the weighted mean value across all classes. Since the angular domain is circular, the mean values are calculated separately for \(sin(\theta)\) and \(cos(\theta)\) to correctly derive the estimated angle.
          </p>

          <figure class="image is-centered" style="margin-bottom: 20px;">
            <img src="./static/images/fusion_comparison.png" 
                 alt="Comparison of Fusion Architectures" 
                 style="width: 80%; display: block; margin-left: auto; margin-right: auto;"/>
            <figcaption class="has-text-centered is-size-7">Illustration of different RGB-D fusion methods: (a) Early fusion, where RGB and depth data are fused at the input level, (b) Late fusion, where features from separate CNNs are concatenated at a later stage, (c) Parallel fusion, where separate CNNs process the data in parallel and the output is fused.</figcaption>
          </figure>

        </div>
      </div>



      <div class="columns is-centered">
      <div class="column is-full-width">

        <h2 class="title is-3">Experiments</h2>
        
        <div class="content has-text-justified">
          <p>
            The evaluation of the relative position estimation involves analyzing the performance of various CNN architectures and assessing the accuracy of the relative radius and angle estimations. The proposed 3D vision sensor demonstrates promising results, as both angle and radius errors are below the predefined industrial error tolerance (1 mm in radius and 10 degrees in angle). Moreover, it was found that the late fusion architecture yields the best performance compared to other fusion strategies.
          </p>
        </div>

        <figure class="image is-centered" style="margin-bottom: 30px;">
            <div class="columns is-centered">
              <div class="column is-half">
                 <img src="./static/images/angle_regression.png" alt="Angle Regression Plot" style="width: 80%;"/>
                 <figcaption class="has-text-centered is-size-7">Comparison of ground truth and estimated values for the Medical Pipe task: (a) Relative angle estimation, (b) Relative radius estimation. The red line represents the ground truth, indicating where the points should ideally lie. The gray lines indicate the first and second margins, encompassing where 95% and 68% of the points are located, respectively.</figcaption>
              </div>
            </div>
          </figure>

          <p>
            We further analyzed the estimation error in the Cartesian coordinate system to understand spatial dependencies. The estimated relative radius error does not correspond to the peg location, suggesting robustness across the workspace. While the estimated relative angle error is higher for small radii, this is expected; however, in such cases, the peg is already initiated inside the hole. The overall spatial distribution confirms that the sensor maintains high accuracy across the insertion plane.
          </p>

          <figure class="image is-centered" style="margin-bottom: 30px;">
            <div class="columns is-centered">
              <div class="column is-half">
                 <img src="./static/images/spatial_error_angle.png" alt="Spatial Angle Error" style="width: 80%;"/>
                 <figcaption class="has-text-centered is-size-7">Estimation errors based on the peg location in the insertion plane: (a) Relative angle error, (b) Relative radius error. The blue and red points indicate estimation error below and above the predefined industrial error respectively (1 [mm] in radius and 10° in angle). </figcaption>
              </div>
            </div>
          </figure>

          <p>
            Since the estimated relative radius serves as an indicative value for the insertion trigger, it was converted into a binary In/Out classification. If the radius exceeds the threshold, the system classifies it as 'Out', prompting location improvement. To minimize false positives, the insertion is initiated only after receiving two consecutive 'In' predictions, reducing the chance of inserting the peg outside the hole to below 1%.
          </p>

          <figure class="image is-centered" style="margin-bottom: 30px;">
            <img src="./static/images/in_out_classification.png" 
                 alt="In/Out Classification Accuracy" 
                 style="width: 80%; display: block; margin-left: auto; margin-right: auto;"/>
            <figcaption class="has-text-centered is-size-7">Analysis of errors in the estimated: (a) Angle estimation error versus actual radius, (b) In/Out binary classification accuracy for the insertion task. In both graphs: The blue and red colors indicate In and Out ground truth samples respectively. The Dark and Bright colors indicate correct and incorrect predictions respectively. (For example, dark red color indicates In ground truth sample that correctly predicted as In)</figcaption>
          </figure>

          <h3 class="title is-4">Robotic Insertion Success Rates</h3>
          <p>
            The crucial test of the 3D vision sensor lies in validating it across the full insertion pipeline. Experiments were conducted on a UR5e robotic cell with both rigid and deformable objects. For the classic PIH task (4.5 mm peg), the vision sensor enhanced the success rate from 83% to 100%. Similarly, for the challenging Medical Pipe task, the success rate increased from 80% to 98%, validating the system's effectiveness in handling deformable objects.
          </p>
        </div>

        <div class="columns is-centered">
          <div class="column is-four-fifths"> <h3 class="title is-4 has-text-centered">Robotic Insertion Success Rates</h3>
            <div class="content">
              <table class="table is-striped is-fullwidth" style="font-size: 0.9em;">
                <thead>
                  <tr class="has-text-centered">
                    <th class="has-text-left" style="border-top: 2px solid #000000; border-bottom: 2px solid #000000;">Task</th>
                    <th style="border-top: 2px solid #000000; border-bottom: 2px solid #000000;">Sensors</th>
                    <th style="border-top: 2px solid #000000; border-bottom: 2px solid #000000;">Radial Error Range ($R_{err}$)</th>
                    <th style="border-top: 2px solid #000000; border-bottom: 2px solid #000000;">Success Rate</th>
                  </tr>
                </thead>
                <tbody class="has-text-centered">
                  <tr>
                    <td class="has-text-left" rowspan="2" style="vertical-align: middle;">4.5mm Peg / 6mm Hole</td>
                    <td>Force Only</td>
                    <td>[1, 2.5] mm</td>
                    <td>83%</td>
                  </tr>
                  <tr style="background-color: #f5f5f5; font-weight: bold;"> <td>Force + Vision</td>
                    <td>[1, 3.0] mm</td>
                    <td>100%</td>
                  </tr>
                  
                  <tr>
                    <td class="has-text-left" rowspan="2" style="vertical-align: middle;">8mm Peg / 10mm Hole</td>
                    <td>Force Only</td>
                    <td>[1.5, 3.5] mm</td>
                    <td>80%</td>
                  </tr>
                  <tr style="background-color: #f5f5f5; font-weight: bold;">
                    <td>Force + Vision</td>
                    <td>[1.5, 10.0] mm</td>
                    <td>100%</td>
                  </tr>

                  <tr>
                    <td class="has-text-left" rowspan="2" style="vertical-align: middle;">16mm Peg / 20mm Hole</td>
                    <td>Force Only</td>
                    <td>[2.5, 8.0] mm</td>
                    <td>51%</td>
                  </tr>
                  <tr style="background-color: #f5f5f5; font-weight: bold;">
                    <td>Force + Vision</td>
                    <td>[4.0, 14.0] mm</td>
                    <td>90%</td>
                  </tr>

                  <tr style="border-bottom: 2px solid #000000;"> <td class="has-text-left" rowspan="2" style="vertical-align: middle;"><strong>Medical Pipe (Deformable)</strong></td>
                    <td>Force Only</td>
                    <td>[1.5, 2.5] mm</td>
                    <td>80%</td>
                  </tr>
                  <tr style="background-color: #f5f5f5; font-weight: bold; border-bottom: 2px solid #000000;">
                    <td>Force + Vision</td>
                    <td>[1.5, 2.5] mm</td>
                    <td>98%</td>
                  </tr>
                </tbody>
              </table>
              <p class="is-size-7">Table 1: Success rate comparison between baseline (Force) and our method (Force + Vision).</p>
            </div>
          </div>
        </div>
      </div>

      </div>
    </div>
    </div>

</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{hazan2024learning,
  title={Learning-Based 3D Vision Sensor for Robotic Peg-In-Hole Insertion of Deformable Objects},
  author={Hazan, Sher and Cohen, Eylon and Schneor, Ronit and Fischer, Anath and Zacksenhouse, Miriam},
  journal={IEEE},
  year={2024}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website borrowed from <a href="https://nerfies.github.io/">NeRFies</a> under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
        </div>
    </div>
  </div>
</footer>

</body>
</html>